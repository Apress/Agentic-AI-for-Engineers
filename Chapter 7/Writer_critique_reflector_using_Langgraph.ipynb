{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7514720a-14bc-4552-ade5-fa03f86f4c73",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STREAM OUTPUT ===\n",
      "\n",
      "{'planner': {'plan': \"Here is a suggested outline for your essay on the differences between LangChain and LangSmith:\\n\\nI. Introduction\\n\\n* Brief overview of LangChain and LangSmith\\n* Thesis statement: While both LangChain and LangSmith are AI-powered language processing tools, they differ in their approaches, capabilities, and applications.\\n\\nII. Background Information\\n\\n* Definition of LangChain: [insert brief description]\\n* Definition of LangSmith: [insert brief description]\\n\\nIII. Key Differences\\n\\n* A. Approach:\\n\\t+ LangChain: [briefly describe LangChain's approach to language processing]\\n\\t+ LangSmith: [briefly describe LangSmith's approach to language processing]\\n* B. Capabilities:\\n\\t+ LangChain: [list specific capabilities of LangChain, e.g., text generation, summarization, etc.]\\n\\t+ LangSmith: [list specific capabilities of LangSmith, e.g., text analysis, sentiment detection, etc.]\\n* C. Applications:\\n\\t+ LangChain: [discuss potential applications of LangChain, e.g., content creation, language translation, etc.]\\n\\t+ LangSmith: [discuss potential applications of LangSmith, e.g., customer service chatbots, market research, etc.]\\n\\nIV. Comparison and Contrast\\n\\n* A. Strengths:\\n\\t+ LangChain: [list strengths of LangChain]\\n\\t+ LangSmith: [list strengths of LangSmith]\\n* B. Weaknesses:\\n\\t+ LangChain: [list weaknesses of LangChain]\\n\\t+ LangSmith: [list weaknesses of LangSmith]\\n\\nV. Conclusion\\n\\n* Restate thesis statement\\n* Summarize key differences between LangChain and LangSmith\\n* Final thoughts: Which tool is more suitable for a particular task or industry?\\n\\nThis outline should provide a clear structure for your essay, allowing you to effectively compare and contrast LangChain and LangSmith.\"}}\n",
      "{'research_plan': {'content': ['In LLM application development, LangChain and LangSmith have become central tools for building and managing large language model-powered solutions. This article compares LangChain and LangSmith, focusing on their core features, integration options, and value for developers in the LLM application space. LangChain is an open-source framework that helps developers create LLM applications efficiently. LangSmith provides tools to debug, monitor, and improve LLM-powered agents, and offers a managed cloud service with a web UI. | LLM Evaluation | Minimal built-in support; developers typically create custom logic or use external tools. LangChain provides building blocks for LLM applications, while LangSmith offers observability and evaluation. LangChain is open-source and free, while LangSmith follows a freemium model for managed services.', 'LangChain is a framework for building applications, while LangSmith is a platform for debugging, evaluating, deploying, and monitoring them.* LangChain became a leading framework for orchestrating LLM workflows, while LangSmith followed to fill important gaps in debugging, evaluation, and production readiness. LangSmith\\xa0is a platform for developing, debugging, evaluating, deploying, and monitoring LLM applications. **LangSmith is a development and deployment platform.** It observes your application through tracing, provides evaluation tools, and hosts your deployments. ## When to Use LangChain or LangSmith **Can you use LangSmith without LangChain?** **Does LangSmith work with frameworks other than LangChain?** **Do I need both LangChain AND LangSmith for production?** You can use LangSmith without LangChain by manually instrumenting your application.', 'This article gives a formal, side-by-side comparison of LangChain vs LangGraph, as well as where LangSmith and LangFlow fit.', \"6. When Should You Use LangChain, LangGraph, or LangSmith? In this blog post, we deep dive into the three powerful tools LangChain vs LangGraph vs LangSmith which are frequently used together but serve distinctly different purposes. While LangChain and LangGraph help you build apps, LangSmith helps you understand and improve them in production. When Should You Use LangChain, LangGraph, or LangSmith? Choosing between LangChain, LangGraph, and LangSmith depends on your project's workflow complexity, debugging needs, and production scale. Yes. LangSmith seamlessly integrates with LangChain and LangGraph, providing monitoring and observability for both simple and complex workflows. While LangChain and LangGraph help you build apps, LangSmith is the best tool for observability and prompt evaluation in production.\", 'If you’re responsible for ensuring your AI models work in production, or you need to frequently debug and monitor your pipelines, Langsmith is your go-to tool. In short, while **Langchain** excels at managing and scaling model workflows, **Langsmith** is designed for those times when you need deep visibility and control over large, complex AI systems in production. But if you’re managing a **complex AI pipeline** with multiple models that need debugging and orchestrating, Langsmith’s capabilities become essential. If you’re debugging complex AI models or managing large-scale workflows with multiple moving parts, **Langsmith’s advanced debugging and orchestration features** will be indispensable. Additionally, if you’re working on **cross-platform model deployments** — say, running models on-prem and in the cloud simultaneously — Langsmith offers better orchestration and monitoring tools to handle the complexity.', 'LangChain is a framework for building applications, while LangSmith is a platform for debugging, evaluating, deploying, and monitoring them.* LangChain became a leading framework for orchestrating LLM workflows, while LangSmith followed to fill important gaps in debugging, evaluation, and production readiness. LangSmith\\xa0is a platform for developing, debugging, evaluating, deploying, and monitoring LLM applications. **LangSmith is a development and deployment platform.** It observes your application through tracing, provides evaluation tools, and hosts your deployments. ## When to Use LangChain or LangSmith **Can you use LangSmith without LangChain?** **Does LangSmith work with frameworks other than LangChain?** **Do I need both LangChain AND LangSmith for production?** You can use LangSmith without LangChain by manually instrumenting your application.']}}\n",
      "{'generate': {'draft': \"Here is the best possible 5-paragraph essay using the provided research and outline:\\n\\nI. Introduction\\n\\nLangChain and LangSmith are two prominent tools in the realm of large language model (LLM) application development. While both share the goal of streamlining LLM-powered solutions, they differ significantly in their approaches, capabilities, and applications. This essay aims to provide a comprehensive comparison between LangChain and LangSmith, highlighting their unique strengths and weaknesses.\\n\\nII. Background Information\\n\\nLangChain is an open-source framework that enables developers to create LLM applications efficiently. It provides building blocks for LLM workflows, allowing users to orchestrate complex AI pipelines with ease. On the other hand, LangSmith is a platform designed for debugging, evaluating, deploying, and monitoring LLM-powered agents. It offers observability tools, evaluation metrics, and managed cloud services with a web UI.\\n\\nIII. Key Differences\\n\\nA key difference between LangChain and LangSmith lies in their approach to language processing. LangChain focuses on building applications using its framework, whereas LangSmith is geared towards understanding and improving LLM-powered systems in production. In terms of capabilities, LangChain excels at managing and scaling model workflows, while LangSmith provides advanced debugging and orchestration features for complex AI pipelines.\\n\\nIV. Comparison and Contrast\\n\\nBoth tools have their strengths. LangChain's open-source nature and flexibility make it an attractive choice for developers seeking to create custom LLM applications. LangSmith's managed cloud services and observability tools, on the other hand, provide valuable insights into production-ready systems. However, LangChain's limitations in debugging and evaluation capabilities are notable, whereas LangSmith's reliance on manual instrumentation may hinder its adoption.\\n\\nV. Conclusion\\n\\nIn conclusion, while both LangChain and LangSmith are essential tools for LLM application development, they cater to different needs and use cases. LangChain is ideal for building custom applications, whereas LangSmith excels at debugging, evaluating, deploying, and monitoring complex AI pipelines. By understanding the unique strengths and weaknesses of each tool, developers can make informed decisions about which tool best suits their project's workflow complexity, debugging needs, and production scale.\", 'revision_number': 2}}\n",
      "{'reflect': {'critique': '**Missing Depth:**\\n\\nWhile your essay provides a good overview of LangChain and LangSmith, it lacks depth in exploring the implications of these differences on real-world applications. For instance, you could have delved deeper into how LangChain\\'s focus on building applications affects its scalability compared to LangSmith\\'s emphasis on debugging and evaluation.\\n\\n**Logic Issues:**\\n\\nYour essay sometimes jumps abruptly from one point to another without providing a clear logical connection. For example, in the \"Key Differences\" section, you transition from discussing approaches to language processing to capabilities without explicitly linking the two. A more cohesive structure would help readers follow your arguments more easily.\\n\\n**Style Improvements:**\\n\\nTo enhance readability and clarity, consider breaking up long sentences into shorter ones. Additionally, use transitional phrases or words (e.g., \"Furthermore,\" \"In contrast\") to guide the reader through your essay\\'s flow.\\n\\n**Structural Improvements:**\\n\\nYour introduction could be stronger by providing a clearer thesis statement that sets the tone for the rest of the essay. Consider reorganizing your sections to create a more logical progression. For instance, you could move the \"Key Differences\" section to after the \"Background Information\" section to provide context before highlighting the differences.\\n\\nHere\\'s an example of how you could revise your introduction:\\n\\n\"While LangChain and LangSmith share the goal of streamlining large language model (LLM) application development, their approaches, capabilities, and applications diverge significantly. This essay aims to explore these differences by examining the unique strengths and weaknesses of each tool, ultimately providing insights for developers seeking to create effective LLM-powered solutions.\"\\n\\nBy addressing these areas, you can strengthen your essay\\'s overall structure, logic, and style, making it more engaging and informative for readers.'}}\n",
      "{'research_critique': {'content': ['In LLM application development, LangChain and LangSmith have become central tools for building and managing large language model-powered solutions. This article compares LangChain and LangSmith, focusing on their core features, integration options, and value for developers in the LLM application space. LangChain is an open-source framework that helps developers create LLM applications efficiently. LangSmith provides tools to debug, monitor, and improve LLM-powered agents, and offers a managed cloud service with a web UI. | LLM Evaluation | Minimal built-in support; developers typically create custom logic or use external tools. LangChain provides building blocks for LLM applications, while LangSmith offers observability and evaluation. LangChain is open-source and free, while LangSmith follows a freemium model for managed services.', 'LangChain is a framework for building applications, while LangSmith is a platform for debugging, evaluating, deploying, and monitoring them.* LangChain became a leading framework for orchestrating LLM workflows, while LangSmith followed to fill important gaps in debugging, evaluation, and production readiness. LangSmith\\xa0is a platform for developing, debugging, evaluating, deploying, and monitoring LLM applications. **LangSmith is a development and deployment platform.** It observes your application through tracing, provides evaluation tools, and hosts your deployments. ## When to Use LangChain or LangSmith **Can you use LangSmith without LangChain?** **Does LangSmith work with frameworks other than LangChain?** **Do I need both LangChain AND LangSmith for production?** You can use LangSmith without LangChain by manually instrumenting your application.', 'This article gives a formal, side-by-side comparison of LangChain vs LangGraph, as well as where LangSmith and LangFlow fit.', \"6. When Should You Use LangChain, LangGraph, or LangSmith? In this blog post, we deep dive into the three powerful tools LangChain vs LangGraph vs LangSmith which are frequently used together but serve distinctly different purposes. While LangChain and LangGraph help you build apps, LangSmith helps you understand and improve them in production. When Should You Use LangChain, LangGraph, or LangSmith? Choosing between LangChain, LangGraph, and LangSmith depends on your project's workflow complexity, debugging needs, and production scale. Yes. LangSmith seamlessly integrates with LangChain and LangGraph, providing monitoring and observability for both simple and complex workflows. While LangChain and LangGraph help you build apps, LangSmith is the best tool for observability and prompt evaluation in production.\", 'If you’re responsible for ensuring your AI models work in production, or you need to frequently debug and monitor your pipelines, Langsmith is your go-to tool. In short, while **Langchain** excels at managing and scaling model workflows, **Langsmith** is designed for those times when you need deep visibility and control over large, complex AI systems in production. But if you’re managing a **complex AI pipeline** with multiple models that need debugging and orchestrating, Langsmith’s capabilities become essential. If you’re debugging complex AI models or managing large-scale workflows with multiple moving parts, **Langsmith’s advanced debugging and orchestration features** will be indispensable. Additionally, if you’re working on **cross-platform model deployments** — say, running models on-prem and in the cloud simultaneously — Langsmith offers better orchestration and monitoring tools to handle the complexity.', 'LangChain is a framework for building applications, while LangSmith is a platform for debugging, evaluating, deploying, and monitoring them.* LangChain became a leading framework for orchestrating LLM workflows, while LangSmith followed to fill important gaps in debugging, evaluation, and production readiness. LangSmith\\xa0is a platform for developing, debugging, evaluating, deploying, and monitoring LLM applications. **LangSmith is a development and deployment platform.** It observes your application through tracing, provides evaluation tools, and hosts your deployments. ## When to Use LangChain or LangSmith **Can you use LangSmith without LangChain?** **Does LangSmith work with frameworks other than LangChain?** **Do I need both LangChain AND LangSmith for production?** You can use LangSmith without LangChain by manually instrumenting your application.']}}\n",
      "{'generate': {'draft': \"Here is the best possible 5-paragraph essay using the provided research:\\n\\nI. Introduction\\n\\nLangChain and LangSmith are two prominent tools in the realm of large language model (LLM) application development. While both tools share a common goal of streamlining LLM-powered solutions, they differ significantly in their approaches, capabilities, and applications. This essay aims to provide a comprehensive comparison between LangChain and LangSmith, highlighting their unique strengths and weaknesses.\\n\\nII. Background Information\\n\\nLangChain is an open-source framework that enables developers to create LLM applications efficiently. It provides building blocks for LLM workflows, allowing users to orchestrate complex AI pipelines with ease. On the other hand, LangSmith is a platform designed for debugging, evaluating, deploying, and monitoring LLM-powered agents. It offers observability and evaluation tools, as well as managed cloud services with a web UI.\\n\\nIII. Key Differences\\n\\nA key difference between LangChain and LangSmith lies in their approach to language processing. LangChain focuses on building applications, providing developers with the necessary tools to create custom workflows. In contrast, LangSmith is designed for debugging, evaluating, and monitoring LLM-powered agents, offering advanced features such as tracing and evaluation tools.\\n\\nIn terms of capabilities, LangChain excels at managing and scaling model workflows, while LangSmith provides deep visibility and control over large, complex AI systems in production. LangChain's capabilities include text generation, summarization, and other language processing tasks, whereas LangSmith offers text analysis, sentiment detection, and other evaluation tools.\\n\\nIV. Comparison and Contrast\\n\\nBoth LangChain and LangSmith have their strengths and weaknesses. LangChain's open-source nature and flexibility make it an attractive option for developers looking to create custom workflows. However, its minimal built-in support may require users to create custom logic or use external tools. LangSmith, on the other hand, offers a managed cloud service with a web UI, making it easier to deploy and monitor LLM-powered agents. However, its freemium model may limit its appeal for developers working on small-scale projects.\\n\\nV. Conclusion\\n\\nIn conclusion, while both LangChain and LangSmith are powerful tools in the realm of LLM application development, they cater to different needs and use cases. LangChain is ideal for developers looking to create custom workflows and manage complex AI pipelines, whereas LangSmith is better suited for those requiring deep visibility and control over large-scale AI systems in production. Ultimately, the choice between LangChain and LangSmith depends on the specific requirements of a project or industry.\", 'revision_number': 3}}\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "# 1. ENVIRONMENT SETUP\n",
    "###############################################################\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "###############################################################\n",
    "# 2. LANGGRAPH 0.6.10 IMPORTS\n",
    "###############################################################\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "###############################################################\n",
    "# 3. LANGUAGE MODEL (LLAMA3 via OLLAMA)\n",
    "###############################################################\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "\n",
    "###############################################################\n",
    "# 4. STATE DEFINITION\n",
    "###############################################################\n",
    "\n",
    "from typing import TypedDict, List\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    task: str\n",
    "    plan: str\n",
    "    draft: str\n",
    "    critique: str\n",
    "    content: List[str]\n",
    "    revision_number: int\n",
    "    max_revisions: int\n",
    "\n",
    "\n",
    "###############################################################\n",
    "# 5. AIR-PACKED PROMPTS\n",
    "###############################################################\n",
    "\n",
    "PLAN_PROMPT = \"\"\"\n",
    "You are an expert essay planner.\n",
    "Create a clean, clear outline for the user's topic.\n",
    "Keep it structured, logical, and helpful.\n",
    "\"\"\"\n",
    "\n",
    "WRITER_PROMPT = \"\"\"\n",
    "You are an expert essay writer.\n",
    "Write the best possible 5-paragraph essay using:\n",
    "\n",
    "- The user's task\n",
    "- The outline\n",
    "- The research below\n",
    "\n",
    "Research:\n",
    "---------\n",
    "{content}\n",
    "\n",
    "If this is a revision, improve clarity, structure, depth, and correctness.\n",
    "\"\"\"\n",
    "\n",
    "REFLECTION_PROMPT = \"\"\"\n",
    "You are a strict teacher.\n",
    "Provide direct critique of the essay:\n",
    "- Missing depth\n",
    "- Logic issues\n",
    "- Style improvements\n",
    "- Structural improvements\n",
    "\"\"\"\n",
    "\n",
    "RESEARCH_PLAN_PROMPT = \"\"\"\n",
    "Return EXACTLY 3 search queries as JSON ONLY:\n",
    "\n",
    "{\n",
    "  \"queries\": [\"q1\", \"q2\", \"q3\"]\n",
    "}\n",
    "\n",
    "Do not explain anything else.\n",
    "\"\"\"\n",
    "\n",
    "RESEARCH_CRITIQUE_PROMPT = \"\"\"\n",
    "Based on the critique, return EXACTLY 3 search queries as JSON ONLY:\n",
    "\n",
    "{\n",
    "  \"queries\": [\"q1\", \"q2\", \"q3\"]\n",
    "}\n",
    "\n",
    "Do not explain anything else.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "###############################################################\n",
    "# 6. TAVILY SETUP\n",
    "###############################################################\n",
    "\n",
    "from tavily import TavilyClient\n",
    "tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "\n",
    "\n",
    "###############################################################\n",
    "# 7. SAFE JSON PARSER FOR OLLAMA OUTPUT\n",
    "###############################################################\n",
    "\n",
    "import json\n",
    "\n",
    "def extract_json(text):\n",
    "    \"\"\"\n",
    "    Extracts the first valid JSON object found in the model output.\n",
    "    If no JSON found, returns empty list.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start = text.index(\"{\")\n",
    "        end = text.rindex(\"}\") + 1\n",
    "        return json.loads(text[start:end])\n",
    "    except:\n",
    "        return {\"queries\": []}\n",
    "\n",
    "\n",
    "\n",
    "###############################################################\n",
    "# 8. NODE DEFINITIONS\n",
    "###############################################################\n",
    "\n",
    "############################\n",
    "# NODE A — Planner\n",
    "############################\n",
    "def planner_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=PLAN_PROMPT),\n",
    "        HumanMessage(content=state[\"task\"])\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"plan\": response.content}\n",
    "\n",
    "\n",
    "############################\n",
    "# NODE B — Research For Plan\n",
    "############################\n",
    "def research_plan_node(state: AgentState):\n",
    "\n",
    "    # Ask LLaMA3 for JSON search queries\n",
    "    prompt = RESEARCH_PLAN_PROMPT + f\"\\n\\nTask: {state['task']}\"\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    data = extract_json(response.content)\n",
    "\n",
    "    content = state.get(\"content\", [])\n",
    "\n",
    "    # Run each query with Tavily\n",
    "    for q in data.get(\"queries\", []):\n",
    "        results = tavily.search(query=q, max_results=2)\n",
    "        for r in results[\"results\"]:\n",
    "            content.append(r[\"content\"])\n",
    "\n",
    "    return {\"content\": content}\n",
    "\n",
    "\n",
    "############################\n",
    "# NODE C — Essay Generation\n",
    "############################\n",
    "def generate_node(state: AgentState):\n",
    "\n",
    "    research_text = \"\\n\\n\".join(state.get(\"content\", []))\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=WRITER_PROMPT.format(content=research_text)),\n",
    "        HumanMessage(content=f\"Task: {state['task']}\\n\\nOutline:\\n{state['plan']}\")\n",
    "    ]\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    return {\n",
    "        \"draft\": response.content,\n",
    "        \"revision_number\": state[\"revision_number\"] + 1\n",
    "    }\n",
    "\n",
    "\n",
    "############################\n",
    "# NODE D — Reflection\n",
    "############################\n",
    "def reflect_node(state: AgentState):\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=REFLECTION_PROMPT),\n",
    "        HumanMessage(content=state[\"draft\"])\n",
    "    ]\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"critique\": response.content}\n",
    "\n",
    "\n",
    "############################\n",
    "# NODE E — Research for Critique\n",
    "############################\n",
    "def research_critique_node(state: AgentState):\n",
    "\n",
    "    prompt = RESEARCH_CRITIQUE_PROMPT + f\"\\n\\nCritique: {state['critique']}\"\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    data = extract_json(response.content)\n",
    "\n",
    "    content = state.get(\"content\", [])\n",
    "\n",
    "    for q in data.get(\"queries\", []):\n",
    "        results = tavily.search(query=q, max_results=2)\n",
    "        for r in results[\"results\"]:\n",
    "            content.append(r[\"content\"])\n",
    "\n",
    "    return {\"content\": content}\n",
    "\n",
    "\n",
    "\n",
    "###############################################################\n",
    "# 9. CONDITIONAL ROUTING\n",
    "###############################################################\n",
    "\n",
    "def should_continue(state: AgentState):\n",
    "    if state[\"revision_number\"] > state[\"max_revisions\"]:\n",
    "        return END\n",
    "    return \"reflect\"\n",
    "\n",
    "\n",
    "\n",
    "###############################################################\n",
    "# 10. BUILD THE GRAPH\n",
    "###############################################################\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "builder.add_node(\"planner\", planner_node)\n",
    "builder.add_node(\"research_plan\", research_plan_node)\n",
    "builder.add_node(\"generate\", generate_node)\n",
    "builder.add_node(\"reflect\", reflect_node)\n",
    "builder.add_node(\"research_critique\", research_critique_node)\n",
    "\n",
    "# Entry\n",
    "builder.set_entry_point(\"planner\")\n",
    "\n",
    "# Normal flow\n",
    "builder.add_edge(\"planner\", \"research_plan\")\n",
    "builder.add_edge(\"research_plan\", \"generate\")\n",
    "\n",
    "builder.add_edge(\"reflect\", \"research_critique\")\n",
    "builder.add_edge(\"research_critique\", \"generate\")\n",
    "\n",
    "# Conditional rewrite loop\n",
    "builder.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    should_continue,\n",
    "    {\n",
    "        END: END,\n",
    "        \"reflect\": \"reflect\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Compile\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "\n",
    "\n",
    "###############################################################\n",
    "# 11. RUN THE GRAPH\n",
    "###############################################################\n",
    "\n",
    "thread = {\"configurable\": {\"thread_id\": \"essay_run_1\"}}\n",
    "\n",
    "print(\"\\n=== STREAM OUTPUT ===\\n\")\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\n",
    "        \"task\": \"What is the difference between LangChain and LangSmith?\",\n",
    "        \"max_revisions\": 2,\n",
    "        \"revision_number\": 1\n",
    "    },\n",
    "    thread\n",
    "):\n",
    "    print(step)\n",
    "\n",
    "\n",
    "\n",
    "###############################################################\n",
    "# 12. GUI SUPPORT (OPTIONAL)\n",
    "###############################################################\n",
    "# from helper import ewriter, writer_gui\n",
    "# MultiAgent = ewriter()\n",
    "# app = writer_gui(MultiAgent.graph)\n",
    "# app.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ddd062-7b0f-4495-918f-05e823afb2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
