{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f07696ca-7a5b-4fef-9e62-ccc1d7015f51",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_research_and_summarize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 408\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    406\u001b[0m     topic \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlarge language model alignment\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 408\u001b[0m     final_content, status \u001b[38;5;241m=\u001b[39m run_pipeline_with_hitl(topic)\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFINAL STATUS: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 325\u001b[0m, in \u001b[0;36mrun_pipeline_with_hitl\u001b[0;34m(topic)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Full pipeline with human-in-the-loop approval.\"\"\"\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m# Run research and summarization phases\u001b[39;00m\n\u001b[0;32m--> 325\u001b[0m summaries \u001b[38;5;241m=\u001b[39m run_research_and_summarize(topic)  \u001b[38;5;66;03m# From previous sections\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# Run judge evaluation\u001b[39;00m\n\u001b[1;32m    328\u001b[0m confidence, recommendation, judgment \u001b[38;5;241m=\u001b[39m run_with_judge(summaries)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_research_and_summarize' is not defined"
     ]
    }
   ],
   "source": [
    "# 05_human_in_loop.py\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "from crewai_tools import SerperDevTool\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "QUALITY_THRESHOLD = 4.0\n",
    "JUDGE_CONFIDENCE_THRESHOLD = 0.8\n",
    "MAX_ITERATIONS = 3\n",
    "\n",
    "search_tool = SerperDevTool()\n",
    "\n",
    "# ============================================================\n",
    "# AGENTS\n",
    "# ============================================================\n",
    "\n",
    "researcher = Agent(\n",
    "    role=\"Senior Research Analyst\",\n",
    "    goal=\"Find the most relevant academic papers on a topic\",\n",
    "    backstory=\"\"\"You are a meticulous researcher who finds high-quality \n",
    "    academic sources from top venues like arXiv, NeurIPS, ICML, and ACL.\n",
    "    You prioritize recent, well-cited papers.\"\"\",\n",
    "    tools=[search_tool],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "summarizer = Agent(\n",
    "    role=\"Technical Writer\",\n",
    "    goal=\"Create clear, accurate summaries of research papers\",\n",
    "    backstory=\"\"\"You are a skilled technical writer who distills complex \n",
    "    research into accessible insights. When given feedback, you incorporate \n",
    "    it thoughtfully to improve your work.\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "critic = Agent(\n",
    "    role=\"Quality Reviewer\",\n",
    "    goal=\"Evaluate summaries for accuracy, clarity, and completeness\",\n",
    "    backstory=\"\"\"You are a demanding but fair editor with high standards.\n",
    "    You provide numerical scores AND specific actionable feedback.\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "judge = Agent(\n",
    "    role=\"Senior Editor\",\n",
    "    goal=\"Make final publication decisions based on overall quality\",\n",
    "    backstory=\"\"\"You are the final gatekeeper before publication. You assess \n",
    "    whether content meets publication standards holisticallyâ€”considering \n",
    "    accuracy, clarity, completeness, and whether it serves the reader's needs. \n",
    "    You assign a confidence score reflecting your certainty that this content \n",
    "    is ready for publication. You are conservative: when in doubt, flag for \n",
    "    human review.\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def extract_scores(critique_output):\n",
    "    \"\"\"Parse the critic's JSON output to extract average score.\"\"\"\n",
    "    try:\n",
    "        json_match = re.search(r'\\[.*\\]', critique_output, re.DOTALL)\n",
    "        if json_match:\n",
    "            evaluations = json.loads(json_match.group())\n",
    "            scores = []\n",
    "            for eval_item in evaluations:\n",
    "                avg = (eval_item.get('accuracy', 0) + \n",
    "                       eval_item.get('clarity', 0) + \n",
    "                       eval_item.get('completeness', 0)) / 3\n",
    "                scores.append(avg)\n",
    "            return sum(scores) / len(scores) if scores else 0\n",
    "    except (json.JSONDecodeError, KeyError, TypeError) as e:\n",
    "        print(f\"âš ï¸  Could not parse critique scores: {e}\")\n",
    "    return 0\n",
    "\n",
    "def extract_feedback(critique_output):\n",
    "    \"\"\"Extract written feedback from critique for next iteration.\"\"\"\n",
    "    try:\n",
    "        json_match = re.search(r'\\[.*\\]', critique_output, re.DOTALL)\n",
    "        if json_match:\n",
    "            evaluations = json.loads(json_match.group())\n",
    "            feedback_items = [eval_item.get('feedback', '') for eval_item in evaluations]\n",
    "            if any(feedback_items):\n",
    "                return \"Previous feedback to address:\\n- \" + \"\\n- \".join(f for f in feedback_items if f)\n",
    "    except (json.JSONDecodeError, KeyError, TypeError):\n",
    "        pass\n",
    "    return \"\"\n",
    "\n",
    "# ============================================================\n",
    "# PHASE 1: RESEARCH\n",
    "# ============================================================\n",
    "\n",
    "def run_research_phase(topic):\n",
    "    \"\"\"Phase 1: Research papers on the given topic.\"\"\"\n",
    "    \n",
    "    research_task = Task(\n",
    "        description=f\"\"\"Search for the top 3 most relevant academic papers on: {topic}\n",
    "        \n",
    "        For each paper, provide:\n",
    "        - Title (exact)\n",
    "        - Authors (first author et al. for papers with many authors)\n",
    "        - Year and venue\n",
    "        - Key findings (verbatim abstract or close paraphrase)\n",
    "        \n",
    "        Focus on influential, well-cited papers from reputable venues.\n",
    "        \"\"\",\n",
    "        expected_output=\"A structured list of 3 papers with full metadata and findings.\",\n",
    "        agent=researcher\n",
    "    )\n",
    "    \n",
    "    research_crew = Crew(\n",
    "        agents=[researcher],\n",
    "        tasks=[research_task],\n",
    "        process=Process.sequential,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    result = research_crew.kickoff()\n",
    "    return result.raw\n",
    "\n",
    "# ============================================================\n",
    "# PHASE 2: SUMMARIZE + CRITIQUE LOOP\n",
    "# ============================================================\n",
    "\n",
    "def run_summarize_critique_loop(research_results):\n",
    "    \"\"\"Phase 2: Iteratively summarize and critique until quality threshold is met.\"\"\"\n",
    "    \n",
    "    previous_feedback = \"\"\n",
    "    final_summaries = \"\"\n",
    "    avg_score = 0\n",
    "    \n",
    "    for iteration in range(1, MAX_ITERATIONS + 1):\n",
    "        print(f\"\\nðŸ“ Iteration {iteration}/{MAX_ITERATIONS}: Summarizing and critiquing...\")\n",
    "        \n",
    "        # Build the summarize task with any previous feedback\n",
    "        feedback_section = f\"\\n\\nIMPORTANT - Address this feedback from the previous iteration:\\n{previous_feedback}\" if previous_feedback else \"\"\n",
    "        \n",
    "        summarize_task = Task(\n",
    "            description=f\"\"\"Based on the research below, create a summary for each paper.\n",
    "            \n",
    "            For each paper, write:\n",
    "            - A 2-3 sentence summary of the main contribution\n",
    "            - The key methodology or approach used\n",
    "            - The primary results or findings\n",
    "            - One sentence on broader implications\n",
    "            \n",
    "            Write for a technical audience. Be precise but accessible.\n",
    "            {feedback_section}\n",
    "            \n",
    "            RESEARCH TO SUMMARIZE:\n",
    "            {research_results}\n",
    "            \"\"\",\n",
    "            expected_output=\"Three paper summaries with contribution, methodology, results, and implications.\",\n",
    "            agent=summarizer\n",
    "        )\n",
    "        \n",
    "        critique_task = Task(\n",
    "            description=\"\"\"Review the summaries and evaluate each one.\n",
    "            \n",
    "            For each summary, rate on a 1-5 scale:\n",
    "            - **accuracy**: Does it correctly represent the paper's claims?\n",
    "            - **clarity**: Is it understandable to the target audience?\n",
    "            - **completeness**: Are the key points covered?\n",
    "            \n",
    "            Return your evaluation as a JSON array:\n",
    "```json\n",
    "            [\n",
    "                {\"paper\": \"Paper Title\", \"accuracy\": X, \"clarity\": X, \"completeness\": X, \n",
    "                 \"feedback\": \"Specific suggestions for improvement\"}\n",
    "            ]\n",
    "```\n",
    "            \n",
    "            Be precise in your scores. 5 means excellent, 3 means acceptable, 1 means poor.\n",
    "            Always provide specific, actionable feedback.\n",
    "            \"\"\",\n",
    "            expected_output=\"JSON array with scores and feedback for each summary.\",\n",
    "            agent=critic\n",
    "        )\n",
    "        \n",
    "        # Run summarizer + critic\n",
    "        refine_crew = Crew(\n",
    "            agents=[summarizer, critic],\n",
    "            tasks=[summarize_task, critique_task],\n",
    "            process=Process.sequential,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        result = refine_crew.kickoff()\n",
    "        final_summaries = result.raw\n",
    "        \n",
    "        # Extract scores from the critique portion\n",
    "        avg_score = extract_scores(final_summaries)\n",
    "        print(f\"\\nðŸ“Š Iteration {iteration} average score: {avg_score:.2f}\")\n",
    "        \n",
    "        if avg_score >= QUALITY_THRESHOLD:\n",
    "            print(f\"âœ… Quality threshold ({QUALITY_THRESHOLD}) met!\")\n",
    "            break\n",
    "        \n",
    "        if iteration < MAX_ITERATIONS:\n",
    "            previous_feedback = extract_feedback(final_summaries)\n",
    "            print(f\"âš ï¸  Score {avg_score:.2f} below threshold ({QUALITY_THRESHOLD}). Refining...\")\n",
    "        else:\n",
    "            print(f\"ðŸš¨ Max iterations reached. Proceeding with current output.\")\n",
    "    \n",
    "    return final_summaries, avg_score\n",
    "\n",
    "# ============================================================\n",
    "# PHASE 3: JUDGE EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "def run_judge_evaluation(summaries):\n",
    "    \"\"\"Phase 3: Run the judge to evaluate final summaries.\"\"\"\n",
    "    \n",
    "    judge_task = Task(\n",
    "        description=f\"\"\"Review the final summaries and provide a publication decision.\n",
    "        \n",
    "        SUMMARIES TO REVIEW:\n",
    "        {summaries}\n",
    "        \n",
    "        For each summary, evaluate:\n",
    "        1. Is the information accurate and well-supported?\n",
    "        2. Is the writing clear and professional?\n",
    "        3. Does it serve the reader's needs?\n",
    "        \n",
    "        Return your judgment as JSON:\n",
    "```json\n",
    "        {{\n",
    "            \"overall_confidence\": 0.XX,\n",
    "            \"recommendation\": \"publish\" or \"revise\" or \"human_review\",\n",
    "            \"paper_evaluations\": [\n",
    "                {{\"paper\": \"Title\", \"confidence\": 0.XX, \"issues\": \"Any concerns or empty string\"}}\n",
    "            ],\n",
    "            \"general_comments\": \"Overall assessment\"\n",
    "        }}\n",
    "```\n",
    "        \n",
    "        Confidence scale:\n",
    "        - 0.9-1.0: Excellent, publish immediately\n",
    "        - 0.8-0.9: Good, acceptable for publication\n",
    "        - 0.7-0.8: Adequate, consider minor revisions\n",
    "        - Below 0.7: Needs work, recommend human review\n",
    "        \"\"\",\n",
    "        expected_output=\"JSON with confidence scores and publication recommendation.\",\n",
    "        agent=judge\n",
    "    )\n",
    "    \n",
    "    judge_crew = Crew(\n",
    "        agents=[judge],\n",
    "        tasks=[judge_task],\n",
    "        process=Process.sequential,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâš–ï¸  Running Judge evaluation...\")\n",
    "    result = judge_crew.kickoff()\n",
    "    \n",
    "    # Parse judge output\n",
    "    try:\n",
    "        json_match = re.search(r'\\{.*\\}', result.raw, re.DOTALL)\n",
    "        if json_match:\n",
    "            judgment = json.loads(json_match.group())\n",
    "            confidence = judgment.get('overall_confidence', 0)\n",
    "            recommendation = judgment.get('recommendation', 'unknown')\n",
    "            \n",
    "            print(f\"\\nðŸ“Š Judge confidence: {confidence:.2%}\")\n",
    "            print(f\"ðŸ“‹ Recommendation: {recommendation}\")\n",
    "            \n",
    "            return confidence, recommendation, judgment\n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        print(f\"âš ï¸  Could not parse judge output: {e}\")\n",
    "    \n",
    "    return 0, \"error\", {}\n",
    "\n",
    "# ============================================================\n",
    "# PHASE 4: HUMAN-IN-THE-LOOP\n",
    "# ============================================================\n",
    "\n",
    "def get_human_approval(content, context=\"\"):\n",
    "    \"\"\"Request human approval for content before proceeding.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ”” HUMAN REVIEW REQUESTED\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if context:\n",
    "        print(f\"\\nðŸ“‹ Context: {context}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*40)\n",
    "    print(\"CONTENT FOR REVIEW:\")\n",
    "    print(\"-\"*40)\n",
    "    print(content)\n",
    "    print(\"-\"*40 + \"\\n\")\n",
    "    \n",
    "    while True:\n",
    "        response = input(\"Approve this content? [y]es / [n]o / [e]dit with feedback: \").lower().strip()\n",
    "        \n",
    "        if response in ['y', 'yes']:\n",
    "            print(\"âœ… Approved by human reviewer.\")\n",
    "            return True, None\n",
    "        elif response in ['n', 'no']:\n",
    "            feedback = input(\"Please provide feedback for revision: \").strip()\n",
    "            print(\"âŒ Rejected by human reviewer.\")\n",
    "            return False, feedback\n",
    "        elif response in ['e', 'edit']:\n",
    "            feedback = input(\"Enter your feedback: \").strip()\n",
    "            print(f\"ðŸ“ Feedback recorded: {feedback}\")\n",
    "            return False, feedback\n",
    "        else:\n",
    "            print(\"âš ï¸  Please enter 'y' (yes), 'n' (no), or 'e' (edit with feedback)\")\n",
    "\n",
    "# ============================================================\n",
    "# COMPLETE PIPELINE WITH HUMAN-IN-THE-LOOP\n",
    "# ============================================================\n",
    "\n",
    "def run_pipeline_with_hitl(topic):\n",
    "    \"\"\"Full pipeline with human-in-the-loop approval.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ðŸ”¬ RESEARCH ASSISTANT PIPELINE (with Human-in-the-Loop)\")\n",
    "    print(f\"ðŸ“š Topic: {topic}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Phase 1: Research\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"ðŸ“š PHASE 1: Research\")\n",
    "    print(\"=\"*40)\n",
    "    research_results = run_research_phase(topic)\n",
    "    print(\"\\nâœ… Research phase complete.\")\n",
    "    \n",
    "    # Phase 2: Summarize + Critique Loop\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"ðŸ“ PHASE 2: Summarization & Critique\")\n",
    "    print(\"=\"*40)\n",
    "    summaries, critique_score = run_summarize_critique_loop(research_results)\n",
    "    print(f\"\\nâœ… Summarization phase complete. Final score: {critique_score:.2f}\")\n",
    "    \n",
    "    # Phase 3: Judge Evaluation\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"âš–ï¸  PHASE 3: Judge Evaluation\")\n",
    "    print(\"=\"*40)\n",
    "    confidence, recommendation, judgment = run_judge_evaluation(summaries)\n",
    "    \n",
    "    # Phase 4: Determine if human review is needed\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"ðŸ‘¤ PHASE 4: Human Review Decision\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    needs_human_review = (\n",
    "        confidence < JUDGE_CONFIDENCE_THRESHOLD or\n",
    "        recommendation == \"human_review\"\n",
    "    )\n",
    "    \n",
    "    if needs_human_review:\n",
    "        print(f\"\\nâš ï¸  Human review required:\")\n",
    "        print(f\"   - Judge confidence: {confidence:.2%} (threshold: {JUDGE_CONFIDENCE_THRESHOLD:.2%})\")\n",
    "        print(f\"   - Recommendation: {recommendation}\")\n",
    "        \n",
    "        context = f\"Judge confidence: {confidence:.2%} | Recommendation: {recommendation}\"\n",
    "        approved, feedback = get_human_approval(summaries, context)\n",
    "        \n",
    "        if approved:\n",
    "            return summaries, \"approved_by_human\"\n",
    "        else:\n",
    "            print(f\"\\nðŸ“ Human feedback for revision: {feedback}\")\n",
    "            return summaries, \"rejected_by_human\"\n",
    "    else:\n",
    "        # High confidence - offer optional human review\n",
    "        print(f\"\\nâœ… Judge confidence ({confidence:.2%}) meets threshold.\")\n",
    "        print(\"   Content is ready for publication.\")\n",
    "        \n",
    "        skip_review = input(\"\\nSkip human review and auto-publish? [y/n]: \").lower().strip()\n",
    "        \n",
    "        if skip_review in ['y', 'yes']:\n",
    "            print(\"âœ… Auto-approved based on high judge confidence.\")\n",
    "            return summaries, \"auto_approved\"\n",
    "        else:\n",
    "            approved, feedback = get_human_approval(summaries, f\"Judge confidence: {confidence:.2%}\")\n",
    "            if approved:\n",
    "                return summaries, \"approved_by_human\"\n",
    "            else:\n",
    "                print(f\"\\nðŸ“ Human feedback: {feedback}\")\n",
    "                return summaries, \"rejected_by_human\"\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    topic = \"large language model alignment\"\n",
    "    \n",
    "    final_content, status = run_pipeline_with_hitl(topic)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ðŸ FINAL STATUS: {status.upper()}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if \"approved\" in status:\n",
    "        print(\"\\nðŸ“° Publishing article...\")\n",
    "        print(\"-\"*40)\n",
    "        print(final_content)\n",
    "        print(\"-\"*40)\n",
    "        print(\"\\nâœ… Article published!\")\n",
    "    else:\n",
    "        print(\"\\nðŸ“‹ Article saved as draft for revision.\")\n",
    "        print(\"-\"*40)\n",
    "        print(final_content)\n",
    "        print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e00e79-9a5e-4cea-a26e-a7cf9ff3a51f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
